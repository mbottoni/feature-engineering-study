{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tf-idf is a simple twist on the bag-of-words approach. It stands for term frequency– inverse document frequency. Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in\n",
    "\n",
    "* Tf-idf transforms word count features through multiplication with a constant. Hence, it is an example of feature scaling, a concept introduced in Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "166033    False\n",
       "166034    False\n",
       "166035    False\n",
       "166036    False\n",
       "166037    False\n",
       "Length: 166038, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load Yelp business data\n",
    "biz_f = open('yelp_academic_dataset_business.json')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "biz_f.close()\n",
    "\n",
    "review_file = open('yelp_academic_dataset_review.json')\n",
    "review_df = pd.DataFrame([json.loads(x) for x in review_file.readlines()])\n",
    "review_file.close()\n",
    "\n",
    "# Pull out only Nightlife and Restaurants businesses\n",
    "two_biz = biz_df[biz_df.apply(lambda x: 'Nightlife' in x['categories'] or\n",
    "                                        'Restaurants' in x['categories'],\n",
    "                                         axis=1)]\n",
    "# Join with the reviews to get all reviews on the two types of business\n",
    "twobiz_reviews = two_biz.merge(review_df, on='business_id', how='inner')\n",
    "# Trim away the features we won't use\n",
    "twobiz_reviews = twobiz_reviews[['business_id',\n",
    "    'name',\n",
    "    'stars_y',\n",
    "    'text',\n",
    "    'categories']]\n",
    "# Create the target column--True for Nightlife businesses, and False otherwise >>> two_biz_reviews['target'] = \\\n",
    "twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'],\n",
    "    axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we used tf-idf as an entry point into a detailed analysis of how feature transformations can affect the model (or not). Tf-idf is an example of feature scaling, so we contrasted its performance with that of another feature scaling method—l2 normalization.\n",
    "\n",
    "The results were not as one might have expected. Tf-idf and l2 normalization do not improve the final classifier’s accuracy above plain bag-of-words. After acquiring some statistical modeling and linear algebra chops, we realize why: neither of them changes the column space of the data matrix.\n",
    "\n",
    "One small difference between the two is that tf-idf can “stretch” the word count as well as “compress” it. In other words, it makes some counts bigger, and others close to zero. Therefore, tf-idf could altogether eliminate uninformative words.\n",
    "\n",
    "Along the way, we also discovered another effect of feature scaling: it improves the condition number of the data matrix, making linear models much faster to train. Both l2 normalization and tf-idf have this effect.\n",
    "\n",
    "To summarize, the lesson is: the right feature scaling can be helpful for classification. The right scaling accentuates the informative words and downweights the common words. It can also improve the condition number of the data matrix. The right scaling is not necessarily uniform column scaling.\n",
    "\n",
    "This story is a wonderful illustration of the difficulty of analyzing the effects of fea‐ ture engineering in the general case. Changing the features affects the training process and the models that ensue. Linear models are the simplest models to under‐ stand, yet it still takes very careful experimentation methodology and a lot of deep mathematical knowledge to tease apart the theoretical and practical impacts. This would be mostly impossible with more complicated models or feature transforma‐ tion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
